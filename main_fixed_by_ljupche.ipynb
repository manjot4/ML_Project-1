{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "from preprocessing import PRI_jet_num_split\n",
    "from preprocessing import standardize, minmax_normalize\n",
    "from preprocessing import clean_nan\n",
    "from preprocessing import map_0_1, map_minus_1_1\n",
    "\n",
    "from cross_validation import get_model, calculate_loss, accuracy, total_cross_validation\n",
    "\n",
    "from implementations import build_poly\n",
    "\n",
    "from scripts.helpers import load_csv_data\n",
    "from scripts.helpers import predict_labels, create_csv_submission\n",
    "\n",
    "from cross_validation import gamma_lambda_selection_cv\n",
    "from cross_validation import plotting_graphs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sort_arr(ids, y_pred):\n",
    "    idx = ids.argsort()\n",
    "    return ids[idx], y_pred[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_fname = \"data/train.csv\"\n",
    "test_fname = \"data/test.csv\"\n",
    "sumbission_fname = \"data/submission.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shapes\n",
      "(250000, 30) (250000,) (250000,)\n",
      "(568238, 30) (568238,) (568238,)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "y_train, X_train, ids_train = load_csv_data(train_fname)\n",
    "y_test, X_test, ids_test = load_csv_data(test_fname)\n",
    "\n",
    "print(\"Shapes\")\n",
    "print(X_train.shape, y_train.shape, ids_train.shape)\n",
    "print(X_test.shape, y_test.shape, ids_test.shape)\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of train subsets: 8\n",
      "Number of test subsets:  8\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# combine_vals = True\n",
    "combine_vals = False\n",
    "\n",
    "train_subsets = PRI_jet_num_split(y_train, X_train, ids_train, combine_vals)\n",
    "test_subsets = PRI_jet_num_split(y_test, X_test, ids_test, combine_vals)\n",
    "\n",
    "print(f\"Number of train subsets: { len(train_subsets) }\")\n",
    "print(f\"Number of test subsets:  { len(test_subsets) }\")\n",
    "print()\n",
    "\n",
    "assert len(train_subsets) == len(test_subsets)\n",
    "num_subsets = len(train_subsets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "ids = np.array([])\n",
    "y_pred = np.array([])\n",
    "\n",
    "# Polynomial max degree\n",
    "max_degree = 2\n",
    "\n",
    "# GD step sizes, regularization factors\n",
    "# lambdas = np.logspace(0, 1, 5) \n",
    "# gammas = np.logspace(0, 1, 5)\n",
    "# gammas, lambdas = [1e-6, 1e-5, 1e-4], [0.0, 0.1, 1.0]\n",
    "\n",
    "# getting the score by total cross_validation\n",
    "# testing_accuracy, testing_loss = [], []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train shape before feature expansion:  (73790, 17)   Test shape: (168195, 17)\n",
      "Train shape after  feature expansion:  (73790, 35)   Test shape: (168195, 35)\n",
      "-1.6468000927981619 2.104501190256358\n",
      "(0, 0)/(4, 5)\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "setting an array element with a sequence.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-19-651ca90f8407>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     50\u001b[0m \u001b[0moptimal_gamma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimal_lambda_\u001b[0m \u001b[0;34m=\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m     gamma_lambda_selection_cv(y_train_subset, X_train_subset, k_fold, initial_w, max_iters, gammas, lambdas,\n\u001b[0;32m---> 52\u001b[0;31m                               seed = seed, batch_size = batch_size, metric = metric, model = model)\n\u001b[0m\u001b[1;32m     53\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'CA_bs:'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mCA_baseline\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Iter:'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m' Best gamma:'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimal_gamma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m' Best lambda:'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimal_lambda_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'\\n'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Desktop/ML_Project-1/cross_validation.py\u001b[0m in \u001b[0;36mgamma_lambda_selection_cv\u001b[0;34m(y, tx, k_fold, initial_w, max_iters, gammas, lambdas, seed, batch_size, metric, model)\u001b[0m\n\u001b[1;32m    170\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"({gamma}, {lambda_})/({len(gammas)}, {len(lambdas)})\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    171\u001b[0m             \u001b[0mloss_tr\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mgamma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlambda_\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss_te\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mgamma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlambda_\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mCA_tr\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mgamma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlambda_\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mCA_te\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mgamma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlambda_\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 172\u001b[0;31m                 \u001b[0;34m=\u001b[0m \u001b[0mtotal_cross_validation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mk_fold\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minitial_w\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_iters\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgammas\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mgamma\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlambdas\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mlambda_\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mseed\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mseed\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    173\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    174\u001b[0m     \u001b[0mloss_idx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mCA_idx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Desktop/ML_Project-1/cross_validation.py\u001b[0m in \u001b[0;36mtotal_cross_validation\u001b[0;34m(y, tx, k_fold, initial_w, max_iters, gamma, lambda_, seed, batch_size, model)\u001b[0m\n\u001b[1;32m    131\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mk\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mk_fold\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    132\u001b[0m         \u001b[0mloss_tr\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss_te\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mca_tr\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mca_te\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 133\u001b[0;31m             \u001b[0;34m=\u001b[0m \u001b[0mcross_validation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mk_indices\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlambda_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minitial_w\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_iters\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgamma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    134\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    135\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mloss_tr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss_te\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mca_tr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mca_te\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Desktop/ML_Project-1/cross_validation.py\u001b[0m in \u001b[0;36mcross_validation\u001b[0;34m(y, tx, k_indices, k, lambda_, initial_w, max_iters, gamma, batch_size, model)\u001b[0m\n\u001b[1;32m    113\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    114\u001b[0m     \u001b[0;31m# calculate loss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 115\u001b[0;31m     \u001b[0mtrain_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcalculate_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mw\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlambda_\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    116\u001b[0m     \u001b[0mtest_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcalculate_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mw\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlambda_\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    117\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Desktop/ML_Project-1/cross_validation.py\u001b[0m in \u001b[0;36mcalculate_loss\u001b[0;34m(model, y, tx, w, lambda_)\u001b[0m\n\u001b[1;32m     90\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     91\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mmodel\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'LOG_REG_GD'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 92\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mcompute_loss_reg_logistic_regression\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mw\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlambda_\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     93\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     94\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mmodel\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'LOG_REG_L1'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Desktop/ML_Project-1/implementations.py\u001b[0m in \u001b[0;36mcompute_loss_reg_logistic_regression\u001b[0;34m(y, tx, w, lambda_)\u001b[0m\n\u001b[1;32m    320\u001b[0m         \u001b[0mThe\u001b[0m \u001b[0mlast\u001b[0m \u001b[0mloss\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mlearned\u001b[0m \u001b[0mweights\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    321\u001b[0m     \"\"\"\n\u001b[0;32m--> 322\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mcompute_loss_logistic_regression\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mw\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mlambda_\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmatmul\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mw\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mT\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    323\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    324\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Desktop/ML_Project-1/implementations.py\u001b[0m in \u001b[0;36mcompute_loss_logistic_regression\u001b[0;34m(y, tx, w)\u001b[0m\n\u001b[1;32m    294\u001b[0m     \"\"\"\n\u001b[1;32m    295\u001b[0m     \u001b[0meps\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1e-5\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 296\u001b[0;31m     \u001b[0mpredictions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msigmoid\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    297\u001b[0m     \u001b[0;31m#print(w)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    298\u001b[0m     \u001b[0;32massert\u001b[0m \u001b[0mpredictions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0meps\u001b[0m \u001b[0;31m# make sure numbers are close to 0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: setting an array element with a sequence."
     ]
    }
   ],
   "source": [
    "i = 0\n",
    "y_train_subset, X_train_subset, ids_train_subset = train_subsets[i]\n",
    "y_test_subset, X_test_subset, ids_test_subset = test_subsets[i]\n",
    "\n",
    "y_train_subset = map_0_1(y_train_subset)\n",
    "X_train_subset, X_test_subset = standardize(X_train_subset, X_test_subset)\n",
    "print(f\"Train shape before feature expansion: {str(X_train_subset.shape):>12}   Test shape: {str(X_test_subset.shape):>12}\")\n",
    "X_train_subset, X_test_subset = build_poly(X_train_subset, max_degree), build_poly(X_test_subset, max_degree)\n",
    "print(f\"Train shape after  feature expansion: {str(X_train_subset.shape):>12}   Test shape: {str(X_test_subset.shape):>12}\")\n",
    "\n",
    "# set n_best_features to X_train_subset.shape[1] if you don't want feature selection\n",
    "n_best_features = round(0.75 * X_train_subset.shape[1])\n",
    "# n_best_features = X_train_subset.shape[1]\n",
    "D = n_best_features\n",
    "N, _ = X_train_subset.shape\n",
    "\n",
    "# accuracy by predicting the majority class in the training dataset\n",
    "CA_baseline = y_train_subset.sum() / N\n",
    "CA_baseline = max(CA_baseline, 1 - CA_baseline)\n",
    "\n",
    "# !!!!!! IMPORTANT\n",
    "# If we increase the reg. factor (lambda), then we must decrease the step size (gamma)\n",
    "# else the model diverges as the step size becomes too big.\n",
    "\n",
    "# FEATURE SELECTION WITH L1 REG.\n",
    "max_iters_fs = 200\n",
    "gamma_fs, lambda_fs = 1e-7, 1e2\n",
    "model_fs = 'LOG_REG_L1'\n",
    "\n",
    "initial_w_fs = np.random.randn(X_train_subset.shape[1])\n",
    "# get_model(model, y, tx, initial_w, max_iters, gamma, lambda_, batch_size)\n",
    "loss_, w_fs = get_model(model_fs, y_train_subset, X_train_subset, initial_w_fs, max_iters_fs, gamma_fs, lambda_fs, 1)\n",
    "features = np.argsort(abs(w_fs))[::-1][:n_best_features]\n",
    "print(w_fs.min(), w_fs.max())\n",
    "\n",
    "# Feature selection\n",
    "X_train_subset, X_test_subset = X_train_subset[:, features], X_test_subset[:, features]\n",
    "\n",
    "# tweak params\n",
    "k_fold = 5\n",
    "max_iters = 2000\n",
    "# gammas, lambdas = [1e-6, 1e-5], [0.0, 1e-3, 1, 10, 100, 500]\n",
    "gammas, lambdas = [5e-6, 1e-5, 1e-4, 1e-3], [0, 1e-2, 1, 10, 100]\n",
    "# gammas, lambdas = [1e-2], [0]\n",
    "seed, batch_size = 17, 1\n",
    "metric, model = 'CA', 'LOG_REG_GD'\n",
    "\n",
    "initial_w = np.random.randn(D)\n",
    "# gamma_lambda_selection_cv(y, tx, k_fold, initial_w, max_iters, gammas, lambdas, seed = 1, metric = 'CA', model = 'LOG_REG_GD')\n",
    "optimal_gamma, optimal_lambda_ = \\\n",
    "    gamma_lambda_selection_cv(y_train_subset, X_train_subset, k_fold, initial_w, max_iters, gammas, lambdas,\n",
    "                              seed = seed, batch_size = batch_size, metric = metric, model = model)\n",
    "print('CA_bs:', CA_baseline)\n",
    "print('Iter:', i, ' Best gamma:', optimal_gamma, ' Best lambda:', optimal_lambda_, '\\n')\n",
    "\n",
    "# get_model(model, y, tx, initial_w, max_iters, gamma, lambda_, batch_size)\n",
    "loss_, w = get_model(model, y_train_subset, X_train_subset, initial_w, max_iters, optimal_gamma, optimal_lambda_, batch_size)\n",
    "\n",
    "\n",
    "# y_pred_test = np.array(map_minus_1_1(predict_labels(w, X_test_subset)))\n",
    "\n",
    "# ids = np.concatenate((ids, ids_test_subset))\n",
    "# y_pred = np.concatenate((y_pred, y_pred_test))\n",
    "\n",
    "# ids, y_pred = sort_arr(ids, y_pred)\n",
    "# create_csv_submission(ids, y_pred, sumbission_fname)\n",
    "\n",
    "# the below example run is when combined=True."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train shape before feature expansion:  (73790, 17)   Test shape: (168195, 17)\n",
      "Train shape after  feature expansion:  (73790, 35)   Test shape: (168195, 35)\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "The truth value of an array with more than one element is ambiguous. Use a.any() or a.all()",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-17-d02a1e11a153>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     34\u001b[0m \u001b[0;31m# get_model(model, y, tx, initial_w, max_iters, gamma, lambda_, batch_size)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m \u001b[0mw_fs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_fs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train_subset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_train_subset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minitial_w_fs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_iters_fs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgamma_fs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlambda_fs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 36\u001b[0;31m \u001b[0mfeatures\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margsort\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mabs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mw_fs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mn_best_features\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     37\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mw_fs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mw_fs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mw_fs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<__array_function__ internals>\u001b[0m in \u001b[0;36margsort\u001b[0;34m(*args, **kwargs)\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/numpy/core/fromnumeric.py\u001b[0m in \u001b[0;36margsort\u001b[0;34m(a, axis, kind, order)\u001b[0m\n\u001b[1;32m   1082\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1083\u001b[0m     \"\"\"\n\u001b[0;32m-> 1084\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_wrapfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'argsort'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkind\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mkind\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0morder\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0morder\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1085\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1086\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/numpy/core/fromnumeric.py\u001b[0m in \u001b[0;36m_wrapfunc\u001b[0;34m(obj, method, *args, **kwds)\u001b[0m\n\u001b[1;32m     59\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     60\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 61\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mbound\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     62\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     63\u001b[0m         \u001b[0;31m# A TypeError occurs if the object does have such a method in its\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: The truth value of an array with more than one element is ambiguous. Use a.any() or a.all()"
     ]
    }
   ],
   "source": [
    "# exp_measure_tr, exp_measure_te = 0, 0\n",
    "\n",
    "# i = 0\n",
    "# y_train_subset, X_train_subset, ids_train_subset = train_subsets[i]\n",
    "# y_test_subset, X_test_subset, ids_test_subset = test_subsets[i]\n",
    "\n",
    "# y_train_subset = map_0_1(y_train_subset)\n",
    "# X_train_subset, X_test_subset = standardize(X_train_subset, X_test_subset)\n",
    "# print(f\"Train shape before feature expansion: {str(X_train_subset.shape):>12}   Test shape: {str(X_test_subset.shape):>12}\")\n",
    "# X_train_subset, X_test_subset = build_poly(X_train_subset, max_degree), build_poly(X_test_subset, max_degree)\n",
    "# print(f\"Train shape after  feature expansion: {str(X_train_subset.shape):>12}   Test shape: {str(X_test_subset.shape):>12}\")\n",
    "\n",
    "# # set n_best_features to X_train_subset.shape[1] if you don't want feature selection\n",
    "# n_best_features = round(0.85 * X_train_subset.shape[1])\n",
    "# # n_best_features = X_train_subset.shape[1]\n",
    "# D = n_best_features\n",
    "# N, _ = X_train_subset.shape\n",
    "\n",
    "# # accuracy by predicting the majority class in the training dataset\n",
    "# CA_one = y_train_subset.sum() / N\n",
    "# CA_zero = 1 - CA_one\n",
    "# CA_baseline = max(CA_zero, CA_one)\n",
    "\n",
    "# # !!!!!! IMPORTANT\n",
    "# # If we increase the reg. factor (lambda), then we must decrease the step size (gamma)\n",
    "# # else the model diverges as the step size becomes too big.\n",
    "\n",
    "# # FEATURE SELECTION WITH L1 REG.\n",
    "# max_iters_fs = 300\n",
    "# gamma_fs, lambda_fs = 1e-7, 1e2\n",
    "# model_fs = 'LOG_REG_L1'\n",
    "\n",
    "# initial_w_fs = np.random.randn(X_train_subset.shape[1])\n",
    "# # get_model(model, y, tx, initial_w, max_iters, gamma, lambda_, batch_size)\n",
    "# w_fs = get_model(model_fs, y_train_subset, X_train_subset, initial_w_fs, max_iters_fs, gamma_fs, lambda_fs, 1)\n",
    "# features = np.argsort(abs(w_fs))[::-1][:n_best_features]\n",
    "# print(w_fs.min(), w_fs.max(), w_fs.mean())\n",
    "\n",
    "# # Feature selection\n",
    "# X_train_subset, X_test_subset = X_train_subset[:, features], X_test_subset[:, features]\n",
    "\n",
    "# # tweak params\n",
    "# k_fold = 5\n",
    "# max_iters = 500\n",
    "# # gammas, lambdas = [1e-6, 1e-5], [0.0, 1e-3, 1, 10, 100, 500]\n",
    "# # gammas[0] ... use it for the large datasets with mass feature\n",
    "# # gammas[1] ... use it for the small datasets without mass feature\n",
    "# # gammas, lambdas = [2e-6, 5e-6], [1, 5] #[[1e-5, 3e-5], [2e-7, 1e-6]], [[0, 1e-2, 1, 10, 50], [0, 1, 1e1, 5e1, 2e2, 1e3]]\n",
    "# gammas, lambdas = [5e-6, 1e-5, 1e-4, 1e-3], [0, 1e-2, 1, 10, 100]\n",
    "# seed, batch_size = 17, 1\n",
    "# metric, model = 'CA', 'LOG_REG_GD'\n",
    "    \n",
    "# initial_w = np.random.randn(D)\n",
    "# # gamma_lambda_selection_cv(y, tx, k_fold, initial_w, max_iters, gammas, lambdas, seed = 1, metric = 'CA', model = 'LOG_REG_GD')\n",
    "# optimal_gamma, optimal_lambda_, measure_tr, measure_te = \\\n",
    "#     gamma_lambda_selection_cv(y_train_subset, X_train_subset, k_fold, initial_w, max_iters, gammas, lambdas,\n",
    "#                       seed = seed, batch_size = batch_size, metric = metric, model = model)\n",
    "# print('CA_bs:', CA_baseline)\n",
    "# print('Iter:', i, ' Best gamma:', optimal_gamma, ' Best lambda:', optimal_lambda_, '\\n')\n",
    "\n",
    "# exp_measure_tr += measure_tr * X_train_subset.shape[0] / X_train.shape[0]\n",
    "# exp_measure_te += measure_te * X_test_subset.shape[0] / X_test.shape[0]\n",
    "# # get_model(model, y, tx, initial_w, max_iters, gamma, lambda_, batch_size)\n",
    "# w = get_model(model, y_train_subset, X_train_subset, initial_w, max_iters, optimal_gamma, optimal_lambda_, batch_size)\n",
    "\n",
    "# print(\"Expected training accuracy / loss:\", exp_measure_tr)\n",
    "# print(\"Expected test accuracy / loss:\", exp_measure_te)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for i in range(num_subsets):\n",
    "#     y_train_subset, X_train_subset, ids_train_subset = train_subsets[i]\n",
    "#     y_test_subset, X_test_subset, ids_test_subset = test_subsets[i]\n",
    "\n",
    "#     y_train_subset = map_0_1(y_train_subset)\n",
    "#     X_train_subset, X_test_subset = standardize(X_train_subset, X_test_subset)\n",
    "#     print(f\"Train shape before feature expansion: {str(X_train_subset.shape):>12}   Test shape: {str(X_test_subset.shape):>12}\")\n",
    "#     X_train_subset, X_test_subset = build_poly(X_train_subset, max_degree), build_poly(X_test_subset, max_degree)\n",
    "#     print(f\"Train shape after  feature expansion: {str(X_train_subset.shape):>12}   Test shape: {str(X_test_subset.shape):>12}\")\n",
    "    \n",
    "#     # set n_best_features to X_train_subset.shape[1] if you don't want feature selection\n",
    "#     n_best_features = round(0.75 * X_train_subset.shape[1])\n",
    "#     # n_best_features = X_train_subset.shape[1]\n",
    "#     D = n_best_features\n",
    "#     N, _ = X_train_subset.shape\n",
    "    \n",
    "#     # accuracy by predicting the majority class in the training dataset\n",
    "#     CA_baseline = y_train_subset.sum() / N\n",
    "#     CA_baseline = max(CA_baseline, 1 - CA_baseline)\n",
    "    \n",
    "#     # !!!!!! IMPORTANT\n",
    "#     # If we increase the reg. factor (lambda), then we must decrease the step size (gamma)\n",
    "#     # else the model diverges as the step size becomes too big.\n",
    "    \n",
    "#     # FEATURE SELECTION WITH L1 REG.\n",
    "#     max_iters_fs = 200\n",
    "#     gamma_fs, lambda_fs = 1e-7, 1e2\n",
    "#     model_fs = 'LOG_REG_L1'\n",
    "    \n",
    "#     initial_w_fs = np.random.randn(X_train_subset.shape[1])\n",
    "#     # get_model(model, y, tx, initial_w, max_iters, gamma, lambda_, batch_size)\n",
    "#     loss_, w_fs = get_model(model_fs, y_train_subset, X_train_subset, initial_w_fs, max_iters_fs, gamma_fs, lambda_fs, 1)\n",
    "#     features = np.argsort(abs(w_fs))[::-1][:n_best_features]\n",
    "#     print(w_fs.min(), w_fs.max())\n",
    "    \n",
    "#     # Feature selection\n",
    "#     X_train_subset, X_test_subset = X_train_subset[:, features], X_test_subset[:, features]\n",
    "    \n",
    "#     # tweak params\n",
    "#     k_fold = 5\n",
    "#     max_iters = 2000\n",
    "#     # gammas, lambdas = [1e-6, 1e-5], [0.0, 1e-3, 1, 10, 100, 500]\n",
    "#     gammas, lambdas = [5e-6, 3e-5], [0, 1e-2, 1, 10, 100]\n",
    "#     # gammas, lambdas = [1e-2], [0]\n",
    "#     seed, batch_size = 17, 1\n",
    "#     metric, model = 'CA', 'LOG_REG_GD'\n",
    "    \n",
    "#     initial_w = np.random.randn(D)\n",
    "#     # gamma_lambda_selection_cv(y, tx, k_fold, initial_w, max_iters, gammas, lambdas, seed = 1, metric = 'CA', model = 'LOG_REG_GD')\n",
    "#     optimal_gamma, optimal_lambda_ = \\\n",
    "#         gamma_lambda_selection_cv(y_train_subset, X_train_subset, k_fold, initial_w, max_iters, gammas, lambdas,\n",
    "#                                   seed = seed, batch_size = batch_size, metric = metric, model = model)\n",
    "#     print('CA_bs:', CA_baseline)\n",
    "#     print('Iter:', i, ' Best gamma:', optimal_gamma, ' Best lambda:', optimal_lambda_, '\\n')\n",
    "    \n",
    "#     # get_model(model, y, tx, initial_w, max_iters, gamma, lambda_, batch_size)\n",
    "#     loss_, w = get_model(model, y_train_subset, X_train_subset, initial_w, max_iters, optimal_gamma, optimal_lambda_, batch_size)\n",
    "\n",
    "    \n",
    "#     y_pred_test = np.array(map_minus_1_1(predict_labels(w, X_test_subset)))\n",
    "\n",
    "#     ids = np.concatenate((ids, ids_test_subset))\n",
    "#     y_pred = np.concatenate((y_pred, y_pred_test))\n",
    "\n",
    "# ids, y_pred = sort_arr(ids, y_pred)\n",
    "# create_csv_submission(ids, y_pred, sumbission_fname)\n",
    "\n",
    "# # the below example run is when combined=True."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from implementations import compute_loss_reg_logistic_regression\n",
    "from implementations import compute_loss_reg_logistic_regression_L1\n",
    "\n",
    "def fn(x):\n",
    "    return 1.0 / (1.0 + np.exp(-x))\n",
    "\n",
    "y_example = np.array([0, 1, 1])\n",
    "x_example, w_start = np.array([[-1, 2, 5, 8], [2, -2, 1, 6], [4, 4, 6, 8]]), np.array([0, 0, 0, 0])\n",
    "#x_example, w_start = np.array([[-1], [2], [4]]), np.array([0])\n",
    "lambda_example = 10\n",
    "\n",
    "w_example_GD = get_model('LOG_REG_GD', y_example, x_example, w_start, 100000, 1e-4, lambda_example, 1)\n",
    "\n",
    "# w_example[0] = 33.379\n",
    "print(w_example_GD)\n",
    "print(fn(x_example.dot(w_example_GD)))\n",
    "print(compute_loss_reg_logistic_regression(y_example, x_example, w_start, lambda_example))\n",
    "print(compute_loss_reg_logistic_regression(y_example, x_example, w_example_GD, lambda_example))\n",
    "print()\n",
    "\n",
    "w_example_L1 = get_model('LOG_REG_L1', y_example, x_example, w_start, 100000, 1e-4, lambda_example, 1)\n",
    "\n",
    "# w_example[0] = 33.379\n",
    "print(w_example_L1)\n",
    "print(fn(x_example.dot(w_example_L1)))\n",
    "print(compute_loss_reg_logistic_regression_L1(y_example, x_example, w_start, lambda_example))\n",
    "print(compute_loss_reg_logistic_regression_L1(y_example, x_example, w_example_L1, lambda_example))\n",
    "print()\n",
    "\n",
    "\n",
    "#gamma_lambda_selection_cv(y_example, x_example, 2, w_start, 100, [1e-5, 5e-1], [0.0, 1e-3],\n",
    "#                                  seed = 1, batch_size = 1, metric = 'CA', model = 'LOG_REG_GD')\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "v = np.array([-1, -2, 3])\n",
    "np.maximum(v, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# USE THIS TO TEST 1 SUBSET\n",
    "\n",
    "i = 0\n",
    "y_train_subset, X_train_subset, ids_train_subset = train_subsets[i]\n",
    "y_test_subset, X_test_subset, ids_test_subset = test_subsets[i]\n",
    "\n",
    "y_train_subset = map_0_1(y_train_subset)\n",
    "X_train_subset, X_test_subset = standardize(X_train_subset, X_test_subset)\n",
    "print(f\"Train shape before feature expansion: {str(X_train_subset.shape):>12}   Test shape: {str(X_test_subset.shape):>12}\\n\")\n",
    "X_train_subset, X_test_subset = build_poly(X_train_subset, max_degree), build_poly(X_test_subset, max_degree)\n",
    "print(f\"Train shape after  feature expansion: {str(X_train_subset.shape):>12}   Test shape: {str(X_test_subset.shape):>12}\\n\")\n",
    "\n",
    "N, D = X_train_subset.shape\n",
    "initial_w = np.random.randn(D)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# need to choose optimal lambda and optimal gamma together\n",
    "k_fold = 5 # can experiment with different numbers\n",
    "max_iters = 50\n",
    "\n",
    "# GD step sizes, regularization factors\n",
    "gammas, lambdas = [1e-5, 1], [0.0, 0.001]\n",
    "optimal_gamma, optimal_lambda_ = gamma_lambda_selection_cv(y_train_subset, X_train_subset, k_fold, initial_w, max_iters, gammas, lambdas)\n",
    "\n",
    "print(optimal_gamma, optimal_lambda_)\n",
    "\n",
    "_, w = reg_logistic_regression(y_train_subset, X_train_subset, optimal_lambda_, initial_w, max_iters, optimal_gamma)\n",
    "y_pred_test = np.array(map_minus_1_1(predict_labels(w, X_test_subset)))\n",
    "\n",
    "ids = np.concatenate((ids, ids_test_subset))\n",
    "y_pred = np.concatenate((y_pred, y_pred_test))\n",
    "\n",
    "ids, y_pred = sort_arr(ids, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
